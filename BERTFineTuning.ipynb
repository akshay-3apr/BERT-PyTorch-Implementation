{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTFineTuning.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjsz8YEubw_H",
        "colab_type": "text"
      },
      "source": [
        "<h1> BERT: <i><b>B</b>idirectional <b>E</b>ncoder <b>R</b>epresentations from <b>T</b>ransformers</i></h1>\n",
        "<h6><i>created by Google AI Language Team in 2018</i></h6>\n",
        "BERT is designed to pre-train deep bidirectional representations from unlabelled text by jointly conditioning on both left and right context in all the layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
        "BERT is trained on unlabelled dataset to achieve state of the art results on 11 individual NLP tasks. And all of this with little fine tuning.\n",
        "\n",
        "Deeply Bidirectional means that BERT learns information from both the left and right side of a token's context during the training.\n",
        "<p>Let's try to understand the concept of left and right context in Deeply Bidirectional</p>\n",
        "<ul>\n",
        "<li>Sentence 1: They exchanged addresses <b>and agreed to keep in touch.</b></li>\n",
        "<li>Sentence 2: <b>People of India will be</b> addressed by Prime Minister today.</li>\n",
        "</ul>\n",
        "\n",
        "If model is trained unidirectional and we try to predict the word <i><b>\"Address\"</b></i> from the above two sentences dataset, then the model will be making error in predicting either of them.\n",
        "\n",
        "<h3> Word Embedding</h3>\n",
        "\n",
        "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/06062705/Word-Vectors.png)\n",
        "\n",
        "Before BERT, NLP community used features based on searching the key terms in the word corpus using Term Frequency.These vectors were used in mathematical and statistical models for classification and regression tasks. There was nothing much that could be done mathematically on term frequency to understand the syntax and semantics of the word in a sentence. Then arrived an era of word embedding. Here every word can be represented in their vector space and words having same meaning were close to each other in vector space. This started from Word2Vec and GloVe. \n",
        "\n",
        "Consider an example:\n",
        "<ul>\n",
        "<li>Sentence 1: Man is related to Woman</b></li>\n",
        "<li>Sentence 2: Then King is related to ...</li>\n",
        "</ul>\n",
        "\n",
        "Above sentence can be explained mathematically as: <b>King - Man + Woman = Queen</b>\n",
        "\n",
        "And this can be achieved using word embeddings.Only issue with such word embeddings was with respect to the information they could store. Word2Vec could store only feedforward information. Resulting in same vectors for similar words used in different context. Such words are know as <b>Polysemy</b> words. To handle polysemy words, prediction led to more complex and deeper LSTM models.\n",
        "\n",
        "The revolutionary NLP architecture, which marked the era of transfer learning in NLP and also letting the model understand the syntax and semantics of a word, ELMo (<i>Embeddings from Language Models</i>) and ULMFit started the new trend. ELMo was then, the answer to the problem of <b>Polysemy</b> words- <i> same words having different meanings based on the context </i>.\n",
        "\n",
        "<h2>Previous NLP model Architectures </h2>\n",
        "\n",
        "![alt text](https://1.bp.blogspot.com/-RLAbr6kPNUo/W9is5FwUXmI/AAAAAAAADeU/5y9466Zoyoc96vqLjbruLK8i_t8qEdHnQCLcBGAs/s640/image3.png)\n",
        "\n",
        "> <i>BERT is deeply bidirectional, OpenAI GPT is unidirectional, and ELMo is shallowly bidirectional.</i>\n",
        "\n",
        "<b>ELMo</b> used weighted sum of forward (<i>context before the token/word</i>) and backward (<i>context after the token/word</i>) pass generated, Intermediate Word vectors from two stacked biLM layers and raw vector generated from character convolutions to produce the final ELMo vector. This helped ELMo look at the past and future context, basically the whole sentence to generate the word vector, resulting in unique vector for Polysemy words.\n",
        "\n",
        "The true power of transfer learning in NLP was unleashed after <b>ULMFiT</b> (<i>Universal Language Model Fine-tuning</i>). The concept revolved around having an Language Model (LM) trained on generic corpora. These LMs were based on same ideology what ImageNet helped to acheive transfer learning in Computer Vision. The stages in transfer learnng <b>pretraining</b> and <b>Fine-tuning</b> which is still followed now started with ULMFiT. In pretraining stage the LMs will be trained to learn generic information over language corpora. When fine-tuning the pretrained model to a downstream task, we will train the model on task specific data. Only the last few layers are the ones that will be trained from scratch. Resulting in better accurracy as the initial layers had generic language understanding and last layers had task specific information. BERT is based on the same idea that fine-tuning a pre-trained language model can help the model achieve better results in the downstream tasks.\n",
        "\n",
        "Following ELMo and UMLFiT on the same ground, came <b>OpenAI GPT</b>(<i>Generative Pre-trained Transformers</i>). OpenAI GPT was based on Transformer based network, as suggested in Google Brains research\n",
        "paper \"[Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\". They replaced the whole LSTM architecture with encoder decoder layer stack. GPT also emphasized the importance of the Transformer framework, which has a simpler architecture and can train faster than an LSTM-based model. It is also able to learn complex patterns in the data by using the Attention mechanism. This started the breaktrough for NLP <i>state of the art</i> frameworks using <b>Transformers</b> which includes BERT.\n",
        "\n",
        "\n",
        "<h2> Coming back to BERT... </h2>\n",
        "BERT surpass the unidirectionality constraints by using a “<i>Masked Language Model (MLM)</i>” pre-training objective. MLM randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. It enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the MLM, BERT also uses a “<i> next sequence prediction</i>” task that jointly pretrains text-pair representations.\n",
        "\n",
        "There are two steps involved in BERT:\n",
        "\n",
        "![](https://www.researchgate.net/profile/Jan_Christian_Blaise_Cruz/publication/334160936/figure/fig1/AS:776030256111617@1562031439583/Overall-BERT-pretraining-and-finetuning-framework-Note-that-the-same-architecture-in.ppm)\n",
        "\n",
        "\n",
        "*   Pre-training: the model is trained on unlabelled data over different pre-training task.\n",
        "*   Fine-tuning: BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labelled data from the downstream task.\n",
        "\n",
        "With the basic understanding of the above two steps, lets deep dive to understand BERT framework.\n",
        "\n",
        "\n",
        "*   <h3>BERT Model Architecture:</h3>\n",
        "BERT Model architecture is a multi-layer bidirectional Transformer encoder-decoder structure.\n",
        "    \n",
        "    ![](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/encoder.png)\n",
        "\n",
        "    *   <b>Encoder</b>: Encoder is composed of a stack of N=6 identical layers. Each layer has two sub layers. The first layer is a multi-head self-attention mechanism, and the second is a position wise fully connected feed-forward network. There is a residual connection around each of the two sub layers, followed by layer normalization.\n",
        "\n",
        "    *   <b>Decoder</b>: Decoder is also composed of N=6 identical layers. Decoder has additional one sub-layer over two sub-layers as present in encoder, which performs multi-head attention over the output of the encoder stack. Similar to encoder we have residual connection around every sub-layers, followed by layer normalization.\n",
        "\n",
        "    *   <b>Attention</b>: Attention is a mechanism to know which word in the context, better contribute to the current word. It is calculated using the dot product between query vector Q and key vector K. The output from attention head is the weighted sum of value vector V, where the weights assigned to each value is computed by a compatibility function of the Query with the corresponding Key.\n",
        "The general formula that sums up the whole process of attention calculation in BERT is:\n",
        "\n",
        "          ![alt text](https://miro.medium.com/proxy/1*V6LGUR-0NmlOGmm0TDAa5g.png)\n",
        "\n",
        "      where, Q is the matrix of queries, K an V matrix represent keys and values.\n",
        "\n",
        "      To fully understand the attention calculation with example, I would request you to go through the [Analytics Vidya blog](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/?utm_source=blog&utm_medium=demystifying-bert-groundbreaking-nlp-framework)\n",
        "\n",
        "2.   <h3> Pre-training BERT:<h3> BERT is pretrained using two unsupervised task:\n",
        "        <ul>\n",
        "        <li> <b>Masked Language Model</b>: In order to train the bidirectional representation, BERT simply mask 15% of the input tokens at random, and then predict those masked tokens. A downside is that it creates a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To deal with this situation, BERT not always replaces the masked words with actual [MASKED] token. The BERT training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, BERT replaces the i-th token with: <ul><li> the [MASK] token 80% of the time</li><li>a random token 10% of the time</li><li>the unchanged i-th token 10% of the time</li></ul>\n",
        "        </li>\n",
        "        <li><b> Next Sentence Prediction (NSP)</b>: In order to train a model that understands sentence relationships, we pre-train for a next sentence prediction task. If there are two sentences A and B, BERT trains on 50% of the time with B as the actual next sentence that follows A (labeled as isNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).\n",
        "        </li>\n",
        "        </ul>\n",
        "3.   <h3>Fine-tuning BERT:</h3>The self-attention mechanism in the Transformer allows BERT to model any downstream task. BERT with self-attention encodes a concatenated text pair, which effectively includes bidirectional cross attention between two sentences. For each task, we simply plug in the task specific inputs and outputs into BERT and fine-tune all the parameters end to end. At the output the token representations are fed into an output layer for token level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as sentimental analysis or entailment.\n",
        "      <!-- <ul><li><b></b></li></ul> -->\n",
        "\n",
        "<h2> Now, Lets start with BERT implementaion using PyTorch: </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Iqn-NPji3pV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#install packages\n",
        "#Implementing BERT using huggingface/transformer PyTorch library\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pfOwZhDjQvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "% matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HirFXkCHjinG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# identify and specify the GPU as the device, later in training loop we will load data into device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "\n",
        "SEED = 2019\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device == torch.device(\"cuda\"):\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rODmOjznCAmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27Pl3LHsCKgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We are using CoLA dataset for single sentence classification\n",
        "## It’s a set of sentences labeled as grammatically correct or incorrect\n",
        "## Link to dataset : https://nyu-mll.github.io/CoLA/\n",
        "## We will use the raw version because we need to use the BERT tokenizer to break the text down into tokens and chunks that the model will recognize.\n",
        "# Upload the train file from your local drive\n",
        "\n",
        "# To upload data from local machine at run time\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "#The below code is when we integrate Google drive to current Colab session.\n",
        "# This is helpful when we want to store the trained model, and later download it to local\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive')\n",
        "if not (os.path.exists('/content/gdrive/My Drive/BERTFineTuning')): \n",
        "  os.mkdir('BERTFineTuning')\n",
        "  os.chdir('/content/gdrive/My Drive/BERTFineTuning')\n",
        "else:\n",
        "  os.chdir('/content/gdrive/My Drive/BERTFineTuning')\n",
        "os.listdir()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W5G-zEEl3ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "df.shape\n",
        "df.sample(10)\n",
        "\n",
        "## create label and sentence list\n",
        "sentences = df.sentence.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEt0hipJcPvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check distribution of data based on labels\n",
        "df.label.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhrt6yNHnY3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Import BERT tokenizer, that is used to convert our text into tokens that corresponds to BERT library\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "## We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "tokenized_texts = [[\"[CLS]\"] + sentence + [\"[SEP]\"] for sentence in tokenized_texts]\n",
        "print(' '.join(tokenized_texts[0]))\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PKXd4NqouRV",
        "colab_type": "text"
      },
      "source": [
        "<h2>BERT requires specifically formatted inputs. For each tokenized input sentence, we need to create:</h2>\n",
        "<ul>\n",
        "<li><b>input ids:</b> a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary\n",
        "</li>\n",
        "<li><b>segment mask:</b> <i>(optional)</i> a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n",
        "</li>\n",
        "<li><b>attention mask:</b> <i>(optional)</i> a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens (we’ll detail this in the next paragraph)\n",
        "</li>\n",
        "<li><b>labels:</b> a single value of 1 or 0. In our task 1 means “grammatical” and 0 means “ungrammatical”\n",
        "</li>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5auMNd0zWCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjIN_zq7zdUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "print(tokenized_texts[0:2])\n",
        "print(input_ids[0:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DioO6uwaW8si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_zeros(source,maxlen,dtype=\"long\",truncating=\"post\",padding=\"post\"):\n",
        "  for i,src in enumerate(source):\n",
        "    if len(src) < maxlen:\n",
        "      #print(\"before changing input_ids[{0}]:{1}\".format(i,input))\n",
        "      src.extend([0] * (maxlen - len(src))) \n",
        "      #print(\"after changing input_ids[{0}]:{1}\".format(i,input))\n",
        "      source[i]=src\n",
        "    elif len(src) > maxlen:\n",
        "      while True:\n",
        "        if len(src) == maxlen:\n",
        "          break\n",
        "        elif len(src) > maxlen:\n",
        "          src = src[:-1]\n",
        "      source[i] = src\n",
        "  \n",
        "  return source"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAcnDqAnzv23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_zeros(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "print(input_ids[1:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkT4xRns0Ffe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Create attention mask\n",
        "attention_masks = []\n",
        "\n",
        "## Create a mask of 1 for all input tokens and 0 for all padding tokens\n",
        "\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "print(attention_masks[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rILpyFmuVVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into a training set and a test set using a stratified k fold\n",
        "train_inputs,validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=SEED,test_size=0.1)\n",
        "train_masks,validation_masks,_,_ = train_test_split(attention_masks,input_ids,random_state=SEED,test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFRo06waKZhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert all our data into torch tensors, required data type for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "917m39AULtC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 16\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "train_data = TensorDataset(train_inputs,train_masks,train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\n",
        "validation_sampler = RandomSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtr-EPdpPIm4",
        "colab_type": "text"
      },
      "source": [
        "<h2>Train Model</h2>\n",
        "<p>Now that our input data is properly formatted, it’s time to fine tune the BERT model.</p>\n",
        "\n",
        "<p>For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on \n",
        "our dataset until that the entire model, end-to-end, is well-suited for our task. The huggingface pytorch implementation includes a set of interfaces \n",
        "designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types \n",
        "designed to accomodate their specific NLP task.</p>\n",
        "\n",
        "<p>We’ll load <i>BertForSequenceClassification</i>. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpA-H1_3RMmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICBOPt5XVxq-",
        "colab_type": "text"
      },
      "source": [
        "<h2>For the purposes of fine-tuning, the authors recommend the following hyperparameter ranges:</h2>\n",
        "<ul>\n",
        "<li>Batch size: 16, 32</li>\n",
        "<li>Learning rate (Adam): 5e-5, 3e-5, 2e-5</li>\n",
        "<li>Number of epochs: 2, 3, 4</li>\n",
        "</li>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeKO4bMYW7oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9WG_MlfXBmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters:\n",
        "lr = 1e-5\n",
        "num_training_steps = len(train_data)\n",
        "num_warmup_steps = len(train_data)/10\n",
        "warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1\n",
        "\n",
        "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "#optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "criterion = torch.nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXfsipSzXH_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P4y_T75XWQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 3\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Training\n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    \n",
        "    #print(loss)\n",
        "    loss = outputs[0]\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    scheduler.step()  # Update learning rate schedule\n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits[0].to('cpu').numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLnfgCvJX6w2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training Evaluation\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGkTQc3rbpPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predict and Evaluate\n",
        "\n",
        "# Upload the test file from your local drive\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6lkDboKb6M5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "tokenized_texts = [[\"[CLS]\"] + sentence + [\"[SEP]\"] for sentence in tokenized_texts]\n",
        "labels = df.label.values\n",
        "\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "# Pad our input tokens\n",
        "input_ids = pad_zeros(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l_Hk05gb8wZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "  #print('logits: ',logits[0].cpu().numpy())\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits[0].cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0axKTdHFb_xC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "matthews_set = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  matthews = matthews_corrcoef(true_labels[i],\n",
        "                 np.argmax(predictions[i], axis=1).flatten())\n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkX0xqcucKHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pXku4dIcNjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matthews_corrcoef(flat_true_labels, flat_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKk7RfZPGHmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from random import choice\n",
        "\n",
        "tok = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "text= \"Today is a \"\n",
        "p=0.5\n",
        "input_ids = torch.tensor(tok.encode(text)).unsqueeze(0)\n",
        "logits = model(input_ids)[0][:, -1]\n",
        "probs = F.softmax(logits, dim=-1).squeeze()\n",
        "idxs = torch.argsort(probs, descending=True)\n",
        "res, cumsum = [], 0.\n",
        "for idx in range(0,10):\n",
        "  pred = tok.convert_ids_to_tokens(int(idxs[idx]))\n",
        "  print(tok.convert_tokens_to_string(pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O4FRn4ULyhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}